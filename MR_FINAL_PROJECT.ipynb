{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nuscenes-devkit &> /dev/null \n",
    "#!pip install pyquaternion &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports TODO(PARTH ): CLEAN IT UP \n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import transform_matrix\n",
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "import torch \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"/home/parth/Desktop/NortheasternCourses/MobileRobotics/FinalProject/Multicam_EgoMotion/nuscenes_mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuScenesDataLoader:\n",
    "    \"\"\"_summary_ NUSCENES DATA LOADER CLASS TO INTRACT WITH NU SCENES \n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path, version=\"v1.0-mini\"):\n",
    "        \"\"\"_summary_ INIT METHOD IS REPONSIBLE TO initialize class variables for data loader\n",
    "\n",
    "        Args:\n",
    "            dataset_path (_type_): _description_ relative path to the dataset , if any path issue is there preferably give absolute path \n",
    "            version (str, optional): _description_. version number Defaults to \"v1.0-mini\".\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.nusc = NuScenes(version=version, dataroot=dataset_path, verbose=True)\n",
    "        self.nusc_can = NuScenesCanBus(dataroot=dataset_path)\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"_summary_ This method is reponsible to create a list of data sample with the required data format from the whole nu scenes data set \n",
    "        \n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_ list of dictionary of above format \n",
    "        \"\"\"\n",
    "        \n",
    "   \n",
    "        \n",
    "        data_scene={}\n",
    "        \n",
    "        for scene in self.nusc.scene:\n",
    "            \n",
    "            sample_token=scene[\"first_sample_token\"]   \n",
    "            scene_name =scene[\"name\"] \n",
    "            data_dict = {}\n",
    "            intrinsics=[]\n",
    "            extrinsics=[]\n",
    "            sample=self.nusc.get('sample', sample_token)\n",
    "            sample_data_tokens = {key: value  for key, value in sample[\"data\"].items() if 'CAM' in key}\n",
    "            for cam, token in sample_data_tokens.items():\n",
    "                sd_record = self.nusc.get('sample_data', token)\n",
    "                cs_record = self.nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "                intrinsics.append(cs_record['camera_intrinsic'])\n",
    "                extrinsics.append(transform_matrix(translation=cs_record['translation'], rotation=Quaternion(cs_record['rotation']), inverse=False))\n",
    "                data_dict[cam]=[]\n",
    "            \n",
    "            data_dict[\"scene\"]=scene_name\n",
    "            data_dict[\"intrinsics\"]=intrinsics\n",
    "            data_dict[\"extrinsics\"]=extrinsics\n",
    "            \n",
    "            data_dict[\"pos\"]=[]\n",
    "            data_dict[\"orientation\"]=[]\n",
    "            data_dict[\"rotation_rate\"]=[]\n",
    "            data_dict[\"vel\"]=[]\n",
    "            data_dict[\"timestamp\"]=[]\n",
    "            data_dict[\"num_of_samples\"]=0\n",
    "                \n",
    "            sample=self.nusc.get('sample', sample_token)\n",
    "            sensor_suite=sample[\"data\"]\n",
    "            \n",
    "            sample_data_tokens = {key: value  for key, value in sensor_suite.items() if 'CAM' in key}\n",
    "            \n",
    "            \n",
    "            record_time_stamp=True\n",
    "            for sensor,init_token in sample_data_tokens.items():\n",
    "                sensor_token=init_token\n",
    "                \n",
    "                while sensor_token != \"\" :\n",
    "                    sd_record = self.nusc.get('sample_data', sensor_token)\n",
    "                    data_dict[sensor].append(self.nusc.get('sample_data', sensor_token)['filename'])\n",
    "                    \n",
    "                    if record_time_stamp:\n",
    "                        data_dict[\"timestamp\"].append(sd_record[\"timestamp\"])\n",
    "                        \n",
    "                    sensor_token=sd_record[\"next\"]\n",
    "                else:\n",
    "                    record_time_stamp=False # record timestep only once\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                sample_token=sample[\"next\"]\n",
    "        \n",
    "\n",
    "            for timestamp in data_dict['timestamp']:\n",
    "                closest_entry = min(self.nusc_can.get_messages(scene_name, 'pose'), key=lambda x: abs(x['utime'] -timestamp))\n",
    "                #print(closest_entry)\n",
    "                data_dict[\"rotation_rate\"].append(closest_entry[\"rotation_rate\"]) \n",
    "                data_dict[\"vel\"].append(closest_entry[\"vel\"])\n",
    "                data_dict[\"pos\"].append(closest_entry[\"pos\"])\n",
    "                data_dict[\"orientation\"].append(closest_entry[\"orientation\"])\n",
    "            \n",
    "            data_dict[\"num_of_samples\"]=min(len(data_dict['timestamp']),200)\n",
    "            data_list=[]\n",
    "            print(data_dict[\"num_of_samples\"])\n",
    "            for i in range(data_dict[\"num_of_samples\"]):\n",
    "                #print(i)\n",
    "                data_list.append({\n",
    "                    \"id\":i,\n",
    "                    \"scene_name\":data_dict[\"scene\"],\n",
    "                    \"timestamp\":data_dict[\"timestamp\"][i],\n",
    "                    \"vel\":data_dict[\"vel\"][i],\n",
    "                    \"rotation_rate\":data_dict[\"rotation_rate\"][i],\n",
    "                    \"pos\":data_dict[\"pos\"][i],\n",
    "                    \"orientation\":data_dict[\"orientation\"][i],\n",
    "                    \"image_list\":[data_dict[key][i] for key, value in sample_data_tokens.items() if 'CAM' in key],\n",
    "                    \"extrinsics\": data_dict[\"extrinsics\"],\n",
    "                    \"intrinsics\":data_dict[\"intrinsics\"],\n",
    "                    \n",
    "                                  })\n",
    "            \n",
    "            data_scene[scene_name]=data_list\n",
    "            \n",
    "        return data_scene\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_loader = NuScenesDataLoader(data_path)\n",
    "data = data_loader.load_data()\n",
    "print(len(data))\n",
    "# Print a sample of the loaded data\n",
    "#print(json.dumps(data[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(data_loader.nusc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Data Format\",data[0])\n",
    "print(data_loader.nusc.scene[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dense_optical_flow(img1, img2,img_vis=False):\n",
    "    rgb_flow = None\n",
    "    flow = None\n",
    "    flow = cv2.calcOpticalFlowFarneback(img1, img2, None, 0.5, 5, 15, 8, 7, 1.5, 0)\n",
    "    #flow = cv2.optflow.createOptFlow_PCAFlow()\n",
    "    #flow = cv2.optflow.createOptFlow_DeepFlow()\n",
    "\n",
    "    # Compute optical flow\n",
    "    #flow = flow.calc(img1, img2, None)\n",
    "    #return flow\n",
    "    \n",
    "    if img_vis:\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        hsv = np.zeros_like(cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR))\n",
    "        hsv[..., 0] = angle * 180 / np.pi / 2\n",
    "        hsv[..., 1] = 255\n",
    "        hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        rgb_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    return flow,rgb_flow\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene1=list(data.keys())[0]\n",
    "img_t0=cv2.imread(data_path+\"/\"+ data[scene1][1][\"image_list\"][0],0)\n",
    "img_t1=cv2.imread(data_path+\"/\"+data[scene1][2][\"image_list\"][0],0)\n",
    "plt.imshow(img_t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow,rgb_flow= dense_optical_flow(img_t0,img_t1,img_vis=True)\n",
    "print(flow.shape,rgb_flow.shape)\n",
    "plt.imshow(rgb_flow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Frame Time Diff\",(data[scene1][7]['timestamp']-data[scene1][6][\"timestamp\"])/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, width, height):\n",
    "    return cv2.resize(image, (width, height))\n",
    "\n",
    "# Function to create a row of images\n",
    "def create_row(images, width, height):\n",
    "    resized_images = [resize_image(img, width, height) for img in images]\n",
    "    return np.hstack(resized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec=True\n",
    "\n",
    "if exec:\n",
    "\n",
    "    for i in range(len(data[scene1])-2):\n",
    "        optical_flow_images=[]\n",
    "        imgs_t0=[]\n",
    "        imgs_t1=[]\n",
    "        for j in range(len(data[scene1][i][\"image_list\"])):\n",
    "            img_t0=cv2.imread(data_path+\"/\"+ data[scene1][i][\"image_list\"][j],0)\n",
    "            img_t1=cv2.imread(data_path+\"/\"+data[scene1][i+1][\"image_list\"][j],0)\n",
    "            flow,rgb_flow= dense_optical_flow(img_t0,img_t1,img_vis=True)\n",
    "            optical_flow_images.append(rgb_flow)\n",
    "            imgs_t0.append(np.dstack([img_t0,img_t0,img_t0]))\n",
    "            imgs_t1.append(np.dstack([img_t1,img_t1,img_t1]))\n",
    "        num_images = 6\n",
    "        image_width = 200  # Adjust as needed\n",
    "        image_height = 200  # Adjust as needed\n",
    "\n",
    "        # Create rows\n",
    "        row_t0 = create_row(imgs_t0, image_width, image_height)\n",
    "        row_t1 = create_row(imgs_t1, image_width, image_height)\n",
    "        row_flow = create_row(optical_flow_images, image_width, image_height)\n",
    "\n",
    "        # Stack rows vertically to form a grid\n",
    "        final_grid = np.vstack([row_t0, row_t1, row_flow])\n",
    "        \n",
    "        cv2.imwrite(\"./results/image_grid{}.jpg\".format(i), final_grid) # update this file path to change directory of writing results \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sparse Optical Flow using Lucas-Kanade method\n",
    "def sparse_optical_flow(img1, img2,img_vis=False):\n",
    "    feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "    lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    p0 = cv2.goodFeaturesToTrack(img1, mask=None, **feature_params)\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(img1, img2, p0, None, **lk_params)\n",
    "    #print(p1.shape,st.shape,err.shape)\n",
    "    # Draw the tracks\n",
    "    mask = np.zeros_like(cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR))\n",
    "    flow = np.zeros_like(np.dstack([img1,img1]))\n",
    "    \n",
    "    for i, (new, old) in enumerate(zip(p1[st == 1], p0[st == 1])):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        flow[int(d),int(c),0]= a-c\n",
    "        flow[int(d),int(c),1]= b-d\n",
    "            \n",
    "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color=(255, 255, 0), thickness=3)\n",
    "        mask = cv2.circle(mask, (int(c), int(d)), 10, color=(255, 0, 0), thickness=-1)\n",
    "    return flow,mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow,mask= sparse_optical_flow(img_t0,img_t1,img_vis=True)\n",
    "bin_mask=np.where(mask[:,:,0]>0,False,True)\n",
    "print(flow.shape,mask.shape)\n",
    "plt.imshow(bin_mask*img_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for optical of each image\n",
    "\n",
    "# Function \n",
    "\"\"\"_summary_\n",
    "    input: \n",
    "    images_t0 : K_cameras, H,W\n",
    "    mages_t1 : K_cameras, H,W\n",
    "    output K_images,N_flow_vector,2  where N_flow_vectors : H*W  if dense optical flow of the image else N_flow_vectors< W*H else  sparese optical flow \n",
    "\"\"\"\n",
    "def compute_optical_flow(images_t0,images_t1, dense=False,vis=True,data_path=\"\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        images_t0 (_type_): _description_\n",
    "        images_t1 (_type_): _description_\n",
    "        dense (bool, optional): _description_. Defaults to False.\n",
    "    \"\"\"\n",
    "    flows=None\n",
    "    masks=None\n",
    "    for img1_name, img2_name in zip(images_t0,images_t1):\n",
    "        img_t0 =cv2.imread(os.path.join(data_path,img1_name),0)\n",
    "        img_t1 =cv2.imread(os.path.join(data_path,img2_name),0)\n",
    "        \n",
    "        if dense:\n",
    "            flow,mask=dense_optical_flow(img_t0,img_t1,vis)\n",
    "            #print(\"In Dense\",flow.shape)\n",
    "        else:\n",
    "            flow,mask=sparse_optical_flow(img_t0,img_t1,vis)\n",
    "        \n",
    "        if flows is not None or masks is not None:\n",
    "            flows=np.concatenate((flows,flow[np.newaxis,...]),axis=0)\n",
    "            if vis:\n",
    "                masks=np.concatenate((masks,mask[np.newaxis,...]),axis=0)\n",
    "        else:\n",
    "            \n",
    "            flows=flow[np.newaxis,...]\n",
    "            \n",
    "            if vis:\n",
    "                \n",
    "                masks=mask[np.newaxis,...]\n",
    "           \n",
    "    return flows,masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows,rgb_masks=compute_optical_flow(data[scene1][0][\"image_list\"],data[scene1][1][\"image_list\"],dense=True,data_path=\"/home/parth/Desktop/NortheasternCourses/MobileRobotics/FinalProject/Multicam_EgoMotion/nuscenes_mini\")\n",
    "print(flows.shape,rgb_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocessing or utility \n",
    "\n",
    "#Dunction convert poitns from pixel plane to normalized camera frame and thresh it at the same time \n",
    "\n",
    "import numpy as np\n",
    "def clean_optical_flow_to_normalized_plane_multiple_cameras(flow, intrinsic_matrices,mag_thresh=0.001):\n",
    "    \"\"\"\n",
    "    Converts optical flow to the normalized image plane for multiple cameras.\n",
    "\n",
    "    Parameters:\n",
    "        flow (numpy.ndarray): Optical flow array of shape (K, H, W, 2), where:\n",
    "                              - K is the number of cameras.\n",
    "                              - H, W are the image dimensions.\n",
    "                              - 2 represents horizontal and vertical flow components.\n",
    "        intrinsic_matrices (numpy.ndarray): Intrinsic matrices of shape (K, 3, 3), where:\n",
    "                                            - K is the number of cameras.\n",
    "                                            - Each 3x3 matrix corresponds to the intrinsic parameters.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized optical flow array of shape (K, H, W, 2).\n",
    "    \"\"\"\n",
    "    # Validate input dimensions\n",
    "    if len(flow.shape) != 4 or len(intrinsic_matrices.shape) != 3:\n",
    "        raise ValueError(\"Flow must have shape (K, H, W, 2) and intrinsics (K, 3, 3).\")\n",
    "    if flow.shape[0] != intrinsic_matrices.shape[0]:\n",
    "        raise ValueError(\"Number of cameras in flow and intrinsics must match.\")\n",
    "    \n",
    "    # Extract dimensions\n",
    "    K, H, W, _ = flow.shape\n",
    "\n",
    "    # Prepare outputs\n",
    "    normalized_flow = np.zeros_like(flow)\n",
    "    normalized_coords = np.zeros_like(flow)\n",
    "    mask =np.zeros((K, H, W), dtype=bool)\n",
    "\n",
    "    # Loop through each camera to process flow and intrinsics\n",
    "    for i in range(K):\n",
    "        # Get intrinsic matrix for the current camera\n",
    "        intrinsic_matrix = intrinsic_matrices[i]\n",
    "        K_inv = np.linalg.inv(intrinsic_matrix)\n",
    "        \n",
    "        # Create grid of pixel coordinates for the current image\n",
    "        x_coords, y_coords = np.meshgrid(np.arange(W), np.arange(H))\n",
    "        ones = np.ones_like(x_coords)\n",
    "\n",
    "        # Homogeneous pixel coordinates\n",
    "        pixel_coords = np.stack((x_coords, y_coords, ones), axis=-1)  # Shape: (H, W, 3)\n",
    "        pixel_coords_flat = pixel_coords.reshape(-1, 3)  # Shape: (H*W, 3)\n",
    "\n",
    "        # Transform pixel coordinates to normalized coordinates\n",
    "        normalized_coords_flat = (K_inv @ pixel_coords_flat.T).T  # Shape: (H*W, 3)\n",
    "        normalized_coords[i,...] = normalized_coords_flat[:, :2].reshape(H, W, 2)  # Shape: (H, W, 2)\n",
    "        \n",
    "\n",
    "        # Normalize optical flow for this camera\n",
    "        #dx_normalized = flow[i, ..., 0] / intrinsic_matrix[0, 0]  # Horizontal flow / fx\n",
    "        #dy_normalized = flow[i, ..., 1] / intrinsic_matrix[1, 1]  # Vertical flow / fy\n",
    "        dx_normalized = (flow[i, ..., 0] ) / intrinsic_matrix[0, 0]  # Horizontal flow\n",
    "        dy_normalized = (flow[i, ..., 1]) / intrinsic_matrix[1, 1] \n",
    "        \n",
    "        magnitude = np.sqrt(flow[i,..., 0]**2 + flow[i,..., 1]**2)\n",
    "        #print(magnitude.shape)\n",
    "        non_zero_magnitude=magnitude[magnitude > mag_thresh]\n",
    "        \n",
    "        lower_threshold = np.max([np.mean(non_zero_magnitude) - 0.5 * np.std(non_zero_magnitude),mag_thresh])\n",
    "        upper_threshold = np.mean(non_zero_magnitude) + 3 * np.std(non_zero_magnitude)\n",
    "        #print(upper_threshold,lower_threshold)\n",
    "        clean_flow_mask = (magnitude > lower_threshold) &(magnitude <upper_threshold)\n",
    "        mask[i]=clean_flow_mask\n",
    "        \n",
    "        normalized_flow[i, ..., 0] = dx_normalized\n",
    "        normalized_flow[i, ..., 1] = dy_normalized\n",
    "\n",
    "    return normalized_coords,normalized_flow ,mask\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_coords,normalized_flow,clean_flow_mask=clean_optical_flow_to_normalized_plane_multiple_cameras(flows,intrinsic_matrices=np.array(data[scene1][0][\"intrinsics\"]))\n",
    "\n",
    "print(normalized_coords.shape,normalized_flow.shape,clean_flow_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask=np.ones_like(clean_flow_mask)*clean_flow_mask\n",
    "for img in clean_flow_mask:\n",
    "    print(np.sum(img),img.shape[0]*img.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(rgb_masks[3],cmap=\"gray\")  # Show the image\n",
    "plt.imshow(clean_flow_mask[3], cmap='jet',alpha=0.1)  # Overlay the mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_zero_flow(normalized_flow):\n",
    "    mask= np.where((normalized_flow[..., 0]**2 + normalized_flow[..., 1]**2),True,False)\n",
    "    return mask\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=non_zero_flow(normalized_flow)\n",
    "masked_normalized_coords=normalized_coords[mask]\n",
    "masked_normalized_flow=normalized_flow[mask]\n",
    "print(len(mask),mask[0].shape, np.sum(mask[0]),900*1600)\n",
    "print(masked_normalized_coords.shape,masked_normalized_flow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install theseus-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import theseus as th\n",
    "#IGNORE THE THESEUS PART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(th.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ( PARTH ) : WRITE GETTER AND SETTERS \n",
    "class MultiCamEgoMotion(object):\n",
    "    def __init__(self):\n",
    "    \n",
    "        #super().__init__(self)\n",
    "        self.flow = None  # [K, H, W, 2]\n",
    "        self.normalized_coords = None # [K, H, W, 2]\n",
    "        self.R_k_list = None  # [K, 3, 3]\n",
    "        self.b_k_list = None  # [K, 3]\n",
    "        self.degenrate_case=False\n",
    "        self.M = torch.zeros((3, 3),device=device)\n",
    "        self.mi = None # [K, H, W, 3]\n",
    "        self.hk= None\n",
    "        self.c = torch.zeros((3),device=device)\n",
    "        #self.w = torch.zeros((3))a\n",
    "        self.t = torch.zeros((3),device=device)\n",
    "        self.w_=torch.zeros((3),device=device)\n",
    "\n",
    "    def compute_data(self,normalized_coords, flows, Extriniscs,w,masks):\n",
    "         # Extract optimization variable w\n",
    "        \n",
    "        if self.mi is None:\n",
    "            self.mi = torch.zeros(torch.concat([normalized_coords,normalized_coords[...,:1]],axis=-1).shape,dtype=torch.float32,device=device) # [K, H, W, 3]\n",
    "            self.hk =torch.zeros(torch.as_tensor(Extriniscs[:,3,0:3]).shape,dtype=torch.float32,device=device) #K,3 dims of bk and hk match but serve diff purpose \n",
    "        self.flow = flows  # [K, H, W, 2]\n",
    "        self.normalized_coords = normalized_coords # [K, H, W, 2]\n",
    "        self.R_k_list = torch.as_tensor(Extriniscs[:,0:3,0:3].clone().detach(),dtype=torch.float32,device=device)  # [K, 3, 3]\n",
    "        self.b_k_list = torch.as_tensor(Extriniscs[:,:3,3].clone().detach(),dtype=torch.float32,device=device)  # [K, 3]\n",
    "        self.w_=w # [3,1]\n",
    "        \n",
    "        \n",
    "        # Initialize M and c\n",
    "        M = torch.zeros((3, 3), dtype=torch.float32,device=device)\n",
    "        c = torch.zeros((3,1), dtype=torch.float32,device=device)\n",
    "\n",
    "        # Loop through all cameras\n",
    "        for k in range(self.flow.shape[0]):  # Iterate over K cameras\n",
    "            \n",
    "            normalized_coords_k=self.normalized_coords[k] # [H, W, 2]\n",
    "            flow_k=self.flow[k] # [H, W, 2]\n",
    "            \n",
    "            R_k = torch.as_tensor(self.R_k_list[k],dtype=torch.float32,device=device)  # [3, 3]\n",
    "            b_k = self.b_k_list[k]  # [3]\n",
    "\n",
    "            # Reconstruct 3D points assuming unit depth\n",
    "            P_k = torch.cat([normalized_coords_k,torch.ones_like(normalized_coords_k[...,:1],device=device)],dim=-1)  # [H, W, 3]\n",
    "            P_k_dot = torch.cat([flow_k, torch.zeros_like(flow_k[..., :1])], dim=-1)  # [H, W, 3]\n",
    "\n",
    "            # Compute omega_k and h_k\n",
    "            omega_k = torch.matmul(R_k.T, w.T).squeeze(-1)  # [3]\n",
    "            \n",
    "            h_k = torch.cross(w, b_k[None])  # [3]\n",
    "            inst_rotation_action= torch.cross(omega_k[None,None].expand_as(P_k),P_k,dim=-1) # [None,None,3] X [H,W,3] -> [H,W,3] it is broaddcasted\n",
    "            combined_flow = P_k_dot + inst_rotation_action # [H,W,3] + [H,W,3] -> [H,W,3] \n",
    "            \n",
    "            influence_of_flow_on_a_point = torch.cross(P_k,combined_flow) # [H,W,3] X [H,W,3] -> [H,W,3] \n",
    "            tranformed_influence_in_ego_frame= torch.matmul(R_k[None,None],influence_of_flow_on_a_point[...,None]).squeeze(-1) # None,None,3,3  @ H,W,3 -> H,W,3\n",
    "            # [H,W,3] @ [3,3].T = H,W,3 \n",
    "            # Compute m_i for each pixel\n",
    "            mki=tranformed_influence_in_ego_frame[masks[k]]\n",
    "            temp_M=torch.einsum(\"ij,ik->jk\", mki, mki) # output 3,3 matrix \n",
    "            M+=temp_M\n",
    "            c += -1 * torch.matmul(temp_M, h_k.T)\n",
    "            self.mi[k][masks[k]]=mki\n",
    "            self.hk[k]=h_k\n",
    "        \n",
    "        self.M=M\n",
    "        self.c=c\n",
    "        \n",
    "        self.t=self.compute_optimal_t(M,c)\n",
    "        \n",
    "        return self.mi , self.c , self.M ,self.hk ,self.t\n",
    "    \n",
    "    def compute_optimal_t(self,M,c):\n",
    "        return torch.matmul(torch.linalg.pinv(M),c)\n",
    "    \n",
    "\n",
    "    def compute_optimal_eigen_val_t_degenerate(self,matrix):\n",
    "    \n",
    "        eigenvalues, eigenvectors = torch.linalg.eig(matrix)\n",
    "        eigenvalues_real = eigenvalues.real\n",
    "        positive_eigenvalues = [(val, idx) for idx, val in enumerate(eigenvalues_real) if val > 0]\n",
    "\n",
    "        #TODO( PARTH): write code to catch this outside \n",
    "        if not positive_eigenvalues:\n",
    "            raise ValueError(\"No positive eigenvalues found.\")\n",
    "        \n",
    "        min_val, min_idx = min(positive_eigenvalues, key=lambda x: x[0].item())\n",
    "        corresponding_vector = eigenvectors[:, min_idx].real\n",
    "        \n",
    "        return min_val, corresponding_vector\n",
    "\n",
    "    \n",
    "    def is_degenarate(self,M,epsilon=1e-6):\n",
    "        return torch.abs(torch.linalg.det(M)) < epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING functions\n",
    "obj=MultiCamEgoMotion()\n",
    "obj.compute_data(torch.zeros((6,400,600,2),device=device),torch.zeros((6,400,600,2),device=device),torch.ones((6,4,4),device=device),torch.zeros((1,3),device=device),torch.ones((6,400,600),dtype=torch.bool,device=device))\n",
    "print(obj.is_degenarate(obj.M))\n",
    "print(obj.compute_optimal_t(obj.M,obj.c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "matrix = torch.tensor([[4.0, 2.0,1.0], [1.0,2.0, 3.0],[1.0,2.0, 3.0]], dtype=torch.float32)\n",
    "min_eigenvalue, eigenvector = obj.compute_optimal_eigen_val_t_degenerate(matrix)\n",
    "\n",
    "print(\"Minimum Positive Eigenvalue:\", min_eigenvalue.item())\n",
    "print(\"Corresponding Eigenvector:\", eigenvector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import jacobian\n",
    "torch.manual_seed(1)\n",
    "size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO(PARTH) Fix Theseus Solver Giving Issues \n",
    "\"\"\"\n",
    "\n",
    "# Define a custom cost function for J1(w)\n",
    "class J1CostFunction(th.CostFunction,MultiCamEgoMotion):\n",
    "    def __init__(self, name,normalized_coords,flows,Extriniscs,cost_weight,w):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            flow_images (torch.Tensor): Optical flow images of shape [K, H, W, 2].\n",
    "            R_k_list (list[torch.Tensor]): List of rotation matrices for each camera.\n",
    "            b_k_list (list[torch.Tensor]): List of translation vectors for each camera.\n",
    "            t (torch.Tensor): Translational velocity vector.\n",
    "        \"\"\"\n",
    "        self.cost_weight = th.ScaleCostWeight(1.0)\n",
    "        th.CostFunction.__init__(self, name=name, cost_weight=self.cost_weight)\n",
    "        MultiCamEgoMotion.__init__(self)\n",
    "        \n",
    "        self.w =w\n",
    "        self.register_optim_vars([\"w\"])\n",
    "        \n",
    "        self.normalized_coords=normalized_coords\n",
    "        self.flow=flows\n",
    "        self.Extriniscs=Extriniscs\n",
    "        self.w_tensor=w.tensor\n",
    "        \n",
    "    \n",
    "    def dim(self):\n",
    "        \"\"\"\n",
    "        Return the dimensionality of the cost function.\n",
    "        \"\"\"\n",
    "        # Example: return dimensionality of the input variable w\n",
    "        return 3\n",
    "\n",
    "    def jacobians(self):\n",
    "        \"\"\"\n",
    "        Compute the Jacobian matrix of the cost function using PyTorch autograd.\n",
    "        \"\"\"\n",
    "        #w = self.w\n",
    "        #w = w.tensor.requires_grad_(True)  # Enable gradient computation for w\n",
    "\n",
    "        # Compute the error\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            J1 = self.error()\n",
    "            \n",
    "            #print(J1)\n",
    "\n",
    "        \n",
    "            grad = torch.autograd.grad(J1, self.w_tensor, retain_graph=False, allow_unused=False)\n",
    "            print(grad)\n",
    "        \n",
    "        \n",
    "        return grad[0], J1  # Directly return the gradient wrt w\n",
    "\n",
    "\n",
    "    def _copy_impl(self, new_name):\n",
    "        \"\"\"\n",
    "        Creates a copy of this cost function object.\n",
    "        \"\"\"\n",
    "        return J1CostFunction(\n",
    "            new_name,\n",
    "            self.normalized_coords.clone(),\n",
    "            self.flow.clone(),\n",
    "            self.Extriniscs.clone(),\n",
    "            self.cost_weight.copy(),self.w.copy()\n",
    "        )\n",
    "\n",
    "    def error(self):\n",
    "        \"\"\"\n",
    "        Compute the residual error for J1(w).\n",
    "        \"\"\"\n",
    "        # Extract optimization variable w\n",
    "        w = self.w\n",
    "        w.tensor.g\n",
    "        self.w_tensor = w.tensor.requires_grad_(True)\n",
    "        \n",
    "        mi ,c , M, hk , t = self.compute_data(self.normalized_coords, self.flow, self.Extriniscs,self.w_tensor)\n",
    "        \n",
    "        if self.is_degenarate(M):\n",
    "            self.degenrate_case=True \n",
    "        \n",
    "        print(self.degenrate_case)\n",
    "        #print(\"M is \", M)\n",
    "        #print(c)\n",
    "        M_reg = M + 1e-6 * torch.eye(M.size(-1), device=M.device)\n",
    "        \n",
    "        \n",
    "        J1 = -1 * torch.dot(c.squeeze(-1), torch.matmul(torch.linalg.pinv(M_reg), c).squeeze(-1))\n",
    "        J1 = torch.sum((mi * hk[:, None, None, :]).sum(dim=-1) ** 2)# mi- K,H,W,3  , hk :K,3\n",
    "                \n",
    "        return J1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Solver Function\n",
    "def solve_ego_motion_single_time_step(images_t0,images_t1,Extrinsics,Intrinsics, w_init):\n",
    "    \"\"\"\n",
    "    Solve the ego-motion estimation problem using Theseus with image format [K, H, W, 2].\n",
    "    \"\"\"\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    #np_img_t0=np.array([cv2.imread(img_name,0) for img_name in images_t0])\n",
    "    #np_img_t1=np.array([[cv2.imread(img_name,0) for img_name in images_t1]])\n",
    "    flow_images,_=compute_optical_flow(images_t0,images_t1,dense=True,vis=True,data_path=\"/home/parth/Desktop/NortheasternCourses/MobileRobotics/FinalProject/Multicam_EgoMotion/nuscenes_mini\")\n",
    "    normalized_coords,normalized_flow,clean_flow_mask = clean_optical_flow_to_normalized_plane_multiple_cameras(flow_images, np.array(Intrinsics))\n",
    "    #flow_images = torch.tensor(flow_images, dtype=torch.float32) # [K, H, W, 2].\n",
    "    normalized_flow=normalized_flow /0.05\n",
    "    \n",
    "    Extrinsics = torch.tensor(Extrinsics, dtype=torch.float32) #[K,4,4]\n",
    "    Intrinsics = torch.tensor(Intrinsics, dtype=torch.float32)\n",
    "    w_init = torch.tensor(w_init, dtype=torch.float32) # [3]\n",
    "    normalized_coords =torch.tensor(normalized_coords, dtype=torch.float32)\n",
    "    normalized_flow = torch.tensor(normalized_flow,dtype=torch.float32)\n",
    "\n",
    "    # Define optimization variable\n",
    "    print(w_init.shape)\n",
    "    w = th.Vector(tensor=w_init, name=\"w\")\n",
    "    #w.tensor = w_init\n",
    "    \n",
    "    \n",
    "\n",
    "    # Create the J1 cost function\n",
    "    j1_cost = J1CostFunction(\n",
    "        name=\"J1\",\n",
    "        normalized_coords=normalized_coords,\n",
    "        flows=normalized_flow,\n",
    "        Extriniscs=Extrinsics,\n",
    "        cost_weight = 1,\n",
    "        w=w,\n",
    "        mask=clean_flow_mask\n",
    "    )\n",
    "\n",
    "    # Create the objective\n",
    "    objective = th.Objective()\n",
    "    objective.add(j1_cost) \n",
    "    \n",
    "    theseus_inputs = {\"w\":w_init}\n",
    "    objective.update(theseus_inputs)\n",
    "    \n",
    "    \n",
    "    print(\"Batch size:\", objective.batch_size)\n",
    "    print(\"Cost weight:\", j1_cost.cost_weight)\n",
    "    print(\"Optim vars:\", j1_cost.optim_vars)\n",
    "    optimizer = th.LevenbergMarquardt(\n",
    "    objective,\n",
    "    linear_solver_cls=th.LUDenseSolver,\n",
    "    linearization_cls=th.DenseLinearization,\n",
    "    max_iterations=50,\n",
    "    step_size=0.5,\n",
    "    damping=0.1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    optimizer.optimize(\n",
    "       max_iterations=5,  # Maximum iterations\n",
    "       verbose=True,  # Print optimization progress\n",
    "    )\n",
    "\n",
    "    print(j1_cost.M)\n",
    "    print(j1_cost.c)\n",
    "    t_optimal=j1_cost.compute_optimal_t(j1_cost.M,j1_cost.c).detach().numpy()\n",
    "    w_optimal= w.tensor.detach()\n",
    "        \n",
    "        \n",
    "    \n",
    "    # # Return the optimized w\n",
    "    return w_optimal , t_optimal\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "w_init=np.array([[0.004477957729250193, 0.02866952493786812, 0.017157725989818573]])\n",
    "w_optimal, t_optimal=solve_ego_motion_single_time_step(data[scene1][0][\"image_list\"][:4],data[scene1][1][\"image_list\"][:3],data[scene1][0][\"extrinsics\"][:3],data[scene1][0][\"intrinsics\"][:3],w_init)\n",
    "print(\"w_optimal\",w_optimal,\"t_optimal\",t_optimal)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[scene1][0][\"vel\"],data[scene1][0][\"rotation_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[scene1][0][\"extrinsics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install adabelief-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTORCH SOLVER PART ( USE THIS INSTEAD OF THESEUS )\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def solve_ego_motion_pytorch(images_t0, images_t1, Extrinsics, Intrinsics, w_init, max_iterations=10, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Solve the ego-motion estimation problem using PyTorch to optimize w.\n",
    "    \"\"\"\n",
    "    # Compute optical flow and preprocess input\n",
    "    flow_images, _ = compute_optical_flow(\n",
    "        images_t0, images_t1, dense=False, vis=True,\n",
    "        data_path=data_path\n",
    "    )\n",
    "    normalized_coords, normalized_flow ,clean_flow_mask= clean_optical_flow_to_normalized_plane_multiple_cameras(\n",
    "        flow_images, np.array(Intrinsics)\n",
    "    )\n",
    "    \n",
    "    obj=MultiCamEgoMotion()\n",
    "\n",
    "    \n",
    "    Extrinsics = torch.tensor(Extrinsics, dtype=torch.float32).to(device=device)  # [K, 4, 4]\n",
    "    Intrinsics = torch.tensor(Intrinsics, dtype=torch.float32).to(device=device)\n",
    "    normalized_coords = torch.tensor(normalized_coords, dtype=torch.float32).to(device=device)\n",
    "    normalized_flow = torch.tensor(normalized_flow, dtype=torch.float32).to(device=device)\n",
    "    w = torch.tensor(w_init, dtype=torch.float32, requires_grad=True,device= device)\n",
    "    clean_flow_mask =torch.tensor(clean_flow_mask,dtype=torch.bool).to(device=device)\n",
    "\n",
    "    \n",
    "    #optimizer = optim.Adam([w], lr=learning_rate,weight_decay=0.1)\n",
    "    #optimizer = SGD(  eps=1e-16, betas=(0.9, 0.999), weight_decay=1e-8)\n",
    "    #optimizer=optimizer = optim.SGD([w], lr=0.01, momentum=0.9)\n",
    "    #optimizer=torch.optim.RMSprop([w], lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0.01, momentum=0.9)\n",
    "    optimizer=torch.optim.Adagrad([w], lr=0.01, weight_decay=0.01)\n",
    "\n",
    "\n",
    "    \n",
    "    def compute_loss(w):\n",
    "        \"\"\"\n",
    "        Custom loss function that computes the cost based on J1.\n",
    "        \"\"\"\n",
    "        mi ,c , M, hk , t= obj.compute_data(normalized_coords, normalized_flow, Extrinsics, w,clean_flow_mask)\n",
    "        M_reg = M + 1e-6 * torch.eye(M.size(-1), device=M.device) # For numerical stability \n",
    "        \n",
    "        #print(\"Is degenerate:\", obj.is_degenarate(M_reg))\n",
    "        if obj.degenrate_case:\n",
    "            J1 = -1 * torch.dot(c.squeeze(-1), torch.matmul(torch.linalg.pinv(M), c).squeeze(-1))\n",
    "            J1 += torch.sum((mi[clean_flow_mask] * hk[:, None, None, :]).sum(dim=-1) ** 2) \n",
    "        else:\n",
    "            min_eigenvalue,translation_vec=obj.compute_optimal_eigen_val_t_degenerate(M)  # mi- K,H,W,3  , hk :K,3\n",
    "            J1=min_eigenvalue\n",
    "        return J1\n",
    "\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        loss = compute_loss(w)  # Compute the loss\n",
    "        loss.backward(retain_graph=True)  # Compute gradients\n",
    "        optimizer.step()  # Update w\n",
    "\n",
    "       \n",
    "        if iteration % 5 == 0 or iteration == max_iterations - 1:\n",
    "            print(f\"Iteration {iteration}: Loss = {loss.item()}, w = {w}\")\n",
    "\n",
    "    t_optimal = obj.compute_optimal_t(obj.M,obj.c)\n",
    "        \n",
    "    final_t=t_optimal.detach().cpu().numpy()\n",
    "        \n",
    "    if obj.is_degenarate(obj.M).detach().cpu().numpy():    \n",
    "        t_optimal = obj.compute_optimal_t(obj.M,obj.c)\n",
    "        \n",
    "        final_t=t_optimal.detach().cpu().numpy()\n",
    "        #print(obj.M,obj.c)\n",
    "    else:\n",
    "        _,t_optimal=obj.compute_optimal_eigen_val_t_degenerate(obj.M)\n",
    "        final_t=t_optimal.detach().cpu().numpy()\n",
    "        \n",
    "    # Return optimized w and t\n",
    "    return obj.is_degenarate(obj.M).detach().cpu().numpy(), w.detach().cpu().numpy(), final_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init=np.array([[0.004477957729250193, 0.02866952493786812, 0.017157725989818573]])\n",
    "degenerate_sol,w_optima_per_frame, t_optimal_per_frame=solve_ego_motion_pytorch(data[scene1][1][\"image_list\"][:],data[scene1][2][\"image_list\"][:],data[scene1][0][\"extrinsics\"][:],data[scene1][0][\"intrinsics\"][:],w_init,max_iterations=55)\n",
    "print(\"degenerate_sol\",degenerate_sol,\"w_optimal\",w_optima_per_frame,\"t_optimal\",t_optimal_per_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=0.1\n",
    "t_optimal_dt = t_optimal_per_frame/dt \n",
    "w_optimal_dt = w_optima_per_frame/dt\n",
    "print(\"vel_optimal\",t_optimal_dt.T, \"w_optimal\",w_optimal_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=4\n",
    "for i in range(samples):\n",
    "    print(\"velocity\", data[scene1][i][\"vel\"],\" roation rate\",data[scene1][i][\"rotation_rate\"],(data[scene1][i+1][\"timestamp\"]-data[scene1][i][\"timestamp\"])/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Delta Translation\",t_optimal_dt[0]-data[scene1][0][\"vel\"],\"Delta Rotation\",w_optimal_dt[0]-data[scene1][0][\"rotation_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_rate_pred=[]\n",
    "translation_rate_pred=[]\n",
    "rotation_rate_gt=[]\n",
    "translation_rate_gt=[]\n",
    "\n",
    "rotation_rate_error=[]\n",
    "translation_rate_error=[]\n",
    "degenerate_case=[]\n",
    "\n",
    "w_init=np.array([[0.00, 0.0, 0.0]])\n",
    "\n",
    "for scene in data.keys():\n",
    "    \n",
    "    w_init=np.array([[0.004477957729250193, 0.02866952493786812, 0.1]])\n",
    "    print(\"Scene\",scene)\n",
    "    for traj_id in range(1,len(data[scene])):\n",
    "        print(\"Trajectory id :\",traj_id)\n",
    "        degenrate_sol,w_optima_per_frame, t_optimal_per_frame=solve_ego_motion_pytorch(data[scene][traj_id-1][\"image_list\"][:],data[scene][traj_id][\"image_list\"][:],data[scene][traj_id][\"extrinsics\"][:],data[scene1][traj_id][\"intrinsics\"][:],w_init,max_iterations=15)\n",
    "        \n",
    "        dt=0.1\n",
    "        t_optimal_dt = t_optimal_per_frame/dt\n",
    "        w_optimal_dt = w_optima_per_frame[0]/dt\n",
    "        \n",
    "        \n",
    "        print(\"w_optimal_frame\",w_optima_per_frame,\"t_optimal_frame\",t_optimal_per_frame)\n",
    "        print(\"w_optimal_dt\",w_optimal_dt,\"t_optimal_dt\",t_optimal_dt)\n",
    "        print(\"Delta Translation\",t_optimal_dt-data[scene1][traj_id][\"vel\"],\"Delta Rotation\",w_optimal_dt-data[scene1][traj_id][\"rotation_rate\"])\n",
    "        \n",
    "        rotation_rate_pred.append(w_optimal_dt)\n",
    "        translation_rate_pred.append(t_optimal_dt)\n",
    "        rotation_rate_gt.append(data[scene1][traj_id][\"rotation_rate\"])\n",
    "        translation_rate_gt.append(data[scene1][traj_id][\"vel\"])\n",
    "        degenerate_case.append(degenrate_sol)\n",
    "        if not degenerate_sol:\n",
    "            w_init=w_optima_per_frame\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(degenerate_case)\n",
    "print(np.sum(degenerate_case),len(degenerate_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,rotation in enumerate(rotation_rate_gt):\n",
    "    print(rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rotation_rate_pred[:])\n",
    "plt.plot( np.array(rotation_rate_pred)[1:,2], marker='o',label=\"pred\")\n",
    "plt.plot( np.array(rotation_rate_gt)[1:,2], marker='o',label=\"gt\")\n",
    "\n",
    "\n",
    "plt.title(\"Rotation Rate Over Indices\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Rotation Rate\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rotation_rate_pred[:])\n",
    "plt.plot( np.array(rotation_rate_gt)[:,2], marker='o')\n",
    "plt.plot( np.array(rotation_rate_gt)[:,1], marker='o')\n",
    "\n",
    "plt.plot( np.array(rotation_rate_gt)[:,0], marker='o')\n",
    "\n",
    "plt.title(\"Rotation Rate Over Indices\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Rotation Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rotation_rate_pred[:])\n",
    "plt.plot( np.array(translation_rate_pred)[:,0], marker='o')\n",
    "\n",
    "plt.plot( np.array(translation_rate_gt)[:,0], marker='o')\n",
    "\n",
    "plt.title(\"Translation Rate Over Indices\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Rotation Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( np.array(translation_rate_gt)[:,2], marker='o')\n",
    "plt.plot( np.array(translation_rate_gt)[:,1], marker='o')\n",
    "\n",
    "plt.plot( np.array(translation_rate_gt)[:,0], marker='o')\n",
    "\n",
    "plt.title(\"Translation Rate Over Indices\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Rotation Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rmse_error(x_pred, x_gt):\n",
    "    \n",
    "    if x_pred.shape != x_gt.shape:\n",
    "        raise ValueError(\"Shapes of x_pred and x_gt must match\")\n",
    "\n",
    "    rmse = np.sqrt(np.mean((x_pred - x_gt) ** 2))\n",
    "    return rmse\n",
    "\n",
    "def distribution_error(x_pred, x_gt, num_bins=10):\n",
    "    \n",
    "    if x_pred.shape != x_gt.shape:\n",
    "        raise ValueError(\"Shapes of x_pred and x_gt must match\")\n",
    "\n",
    "    abs_diff = np.linalg.norm(x_pred - x_gt, axis=1)\n",
    "    hist, bin_edges = np.histogram(abs_diff, bins=num_bins, density=True)\n",
    "    return bin_edges, hist\n",
    "\n",
    "def plot_distributions(bin_edges, hist_values, title=\"Distribution Error\", xlabel=\"Error\", ylabel=\"Frequency\"):\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(bin_edges[:-1], hist_values, width=np.diff(bin_edges), align=\"edge\", alpha=0.7, edgecolor=\"black\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse_error( np.nan_to_num(np.array(rotation_rate_gt)), np.nan_to_num(np.array(rotation_rate_pred))[:,:]))\n",
    "print(rmse_error( np.nan_to_num(np.array(rotation_rate_gt)[:,0]), np.nan_to_num(np.array(rotation_rate_pred))[:,0]))\n",
    "print(rmse_error( np.nan_to_num(np.array(rotation_rate_gt)[:,1]), np.nan_to_num(np.array(rotation_rate_pred))[:,1]))\n",
    "print(rmse_error( np.nan_to_num(np.array(rotation_rate_gt)[:,2]), np.nan_to_num(np.array(rotation_rate_pred))[:,2]))\n",
    "\n",
    "\n",
    "print(rmse_error( np.nan_to_num(np.array(translation_rate_gt)[:,:]), np.nan_to_num(np.array(translation_rate_pred))[:,]))\n",
    "print(rmse_error( np.nan_to_num(np.array(translation_rate_gt)[:,0]), np.nan_to_num(np.array(translation_rate_pred))[:,0]))\n",
    "print(rmse_error( np.nan_to_num(np.array(translation_rate_gt)[:,1]), np.nan_to_num(np.array(translation_rate_pred))[:,1]))\n",
    "print(rmse_error( np.nan_to_num(np.array(translation_rate_gt)[:,2]), np.nan_to_num(np.array(translation_rate_pred))[:,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_pred = np.array(rotation_rate_gt)\n",
    "x_gt = np.array(rotation_rate_pred)\n",
    "rmse = rmse_error(x_pred, x_gt)\n",
    "print(\"RMSE:\", rmse)\n",
    "bin_edges, hist_values = distribution_error(x_pred, x_gt, num_bins=20)\n",
    "plot_distributions(bin_edges, hist_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def compute_cross_product(P_k, P_k_dot, omega_k):\n",
    "    # Compute cross products\n",
    "    cross_omega_Pk = np.cross(omega_k, P_k)\n",
    "    inner_term = P_k_dot + cross_omega_Pk\n",
    "    cross_Pk_inner = np.cross(P_k, inner_term)\n",
    "    return cross_Pk_inner\n",
    "\n",
    "def compute_Pk_dot(P_k, omega_k, t_k):\n",
    "    # Compute P_k_dot using equation (3): P_k_dot = -omega_k x P_k - t_k\n",
    "    cross_omega_Pk = np.cross(omega_k, P_k)\n",
    "    P_k_dot = -cross_omega_Pk - t_k\n",
    "    return P_k_dot\n",
    "\n",
    "# Define example vectors\n",
    "P_k = np.array([1.2, -3.1, 1.0])      # Example 3D point\n",
    "omega_k = np.array([0.1, 0.2, 0.3])  # Angular velocity\n",
    "t_k = np.array([0.5, -0.5, 0.5])     # Translational velocity\n",
    "\n",
    "# Compute P_k_dot using the equation\n",
    "P_k_dot = compute_Pk_dot(P_k, omega_k, t_k)\n",
    "\n",
    "# Define a range of scalar multipliers for visualization\n",
    "t_values = np.linspace(-1, 1, 100)\n",
    "\n",
    "# Compute results for the cross product in each case\n",
    "results = np.array([compute_cross_product(P_k, P_k_dot, omega_k) * t for t in t_values])\n",
    "#print(results)\n",
    "#print(t_values)\n",
    "\n",
    "# Extract x, y, z components for 3D plotting\n",
    "x_values = results[:, 0]\n",
    "y_values = results[:, 1]\n",
    "z_values = results[:, 2]\n",
    "\n",
    "# Plot the 3D results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the cross product\n",
    "ax.plot(x_values, y_values, z_values, label='Pk x [ Pk_dot + omega_k x Pk ]', color='blue')\n",
    "\n",
    "# Add vectors P_k, omega_k, and t_k to the plot\n",
    "origin = np.array([0, 0, 0])\n",
    "ax.quiver(*origin, *P_k, color='green', label='P_k', arrow_length_ratio=0.1)\n",
    "ax.quiver(*origin, *omega_k, color='red', label='omega_k', arrow_length_ratio=0.1)\n",
    "ax.quiver(*origin, *t_k, color='purple', label='t_k', arrow_length_ratio=0.1)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_title('3D Visualization of Cross Product with Vectors')\n",
    "ax.set_xlabel('X Component')\n",
    "ax.set_ylabel('Y Component')\n",
    "ax.set_zlabel('Z Component')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
